# Corrected Bert Code
from transformers import TrainingArguments, Trainer
from datasets import load_dataset
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Load dataset
dataset = load_dataset("go_emotions", "simplified")

# Load tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenize function
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Tokenize datasets
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Correct label formatting
def process_labels(example):
    example['labels'] = example['labels'][0]  # Take only the first label
    return example

tokenized_datasets = tokenized_datasets.map(process_labels)

# Verify label types
print(type(tokenized_datasets["train"]["labels"][0]))
print(tokenized_datasets["train"]["labels"][0:10])

# Set format for PyTorch
tokenized_datasets.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# Model and training arguments
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=28) #go_emotions simplified has 28 labels.

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
)

# Train and evaluate
trainer.train()
trainer.evaluate()
